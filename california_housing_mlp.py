# -*- coding: utf-8 -*-
"""California_Housing_Regression_MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gHY8yeMYbvbNqCpf0ABtZkNFNpS-ncfx
"""

import torch
from torch import nn
from torch.utils.data import DataLoader
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter()

class CaliforniaDataset(torch.utils.data.Dataset):
  '''
  Prepare the California dataset for regression
  '''

  def __init__(self, X, y, scale_data=True):
    if not torch.is_tensor(X) and not torch.is_tensor(y):
      # Apply scaling if necessary
      if scale_data:
          X = StandardScaler().fit_transform(X)
      self.X = torch.from_numpy(X)
      self.y = torch.from_numpy(y)

  def __len__(self):
      return len(self.X)

  def __getitem__(self, i):
      return self.X[i], self.y[i]


class MLP(nn.Module):
  '''
    Multilayer Perceptron for regression.
  '''
  def __init__(self):
    super().__init__()
    self.layers = nn.Sequential(
      nn.Linear(8, 64),
      nn.ReLU(),
      nn.Linear(64, 32),
      nn.ReLU(),
      nn.Linear(32, 1)
    )


  def forward(self, x):
    '''
      Forward pass
    '''
    return self.layers(x)


if __name__ == '__main__':

  # Set fixed random number seed
  torch.manual_seed(42)

  housing = fetch_california_housing()
  X = housing['data']
  y = housing['target']
  print(f"the housing dataset X shape is {X.shape}")

  # train test split
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

  # Prepare California dataset
  train_dataset = CaliforniaDataset(X_train, y_train)
  val_dataset = CaliforniaDataset(X_test, y_test)

  trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=1)
  valloader = torch.utils.data.DataLoader(val_dataset, batch_size=10, shuffle=False, num_workers=1)

  # Initialize the MLP
  mlp = MLP()

  # Define the loss function and optimizer
  # loss_function = nn.L1Loss()
  loss_function = nn.MSELoss()
  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)

  # Run the training loop
  for epoch in range(0, 6): # 5 epochs at maximum

    # Print epoch
    print(f'Starting epoch {epoch+1}')

    # Set current loss value
    current_loss = 0.0
    current_val_loss = 0.0

    # Iterate over the DataLoader for training data
    for i, data in enumerate(trainloader, 0):

      # Get and prepare inputs
      inputs, targets = data
      inputs, targets = inputs.float(), targets.float()
      targets = targets.reshape((targets.shape[0], 1))

      # Zero the gradients
      optimizer.zero_grad()

      # Perform forward pass
      outputs = mlp(inputs)

      # Compute loss
      loss = loss_function(outputs, targets)

      # Perform backward pass
      loss.backward()

      # Perform optimization
      optimizer.step()

      # Print statistics
      current_loss += loss.item()
      writer.add_scalar("train loss x epoch", current_loss/len(trainloader), epoch)
      if i % 10 == 0:
          print('Loss after mini-batch %5d: %.3f' %
                (i + 1, current_loss / 500))
          current_loss = 0.0


    # ---------------------------------------------
    # Validation data loss calculation
    torch.no_grad()
    mlp.eval()

    for i, data in enumerate(valloader, 0):

      # Get and prepare inputs
      inputs, targets = data
      inputs, targets = inputs.float(), targets.float()
      targets = targets.reshape((targets.shape[0], 1))

      # Perform forward pass
      outputs = mlp(inputs)

      # Compute loss
      loss = loss_function(outputs, targets)

      # Print statistics
      current_val_loss += loss.item()
      writer.add_scalar("Val loss x epoch", current_val_loss/len(valloader), epoch)
      if i % 10 == 0:
          print('Validation Loss after mini-batch %5d: %.3f' %
                (i + 1, current_val_loss / 500))
          current_val_loss = 0.0


  # Process is complete.
  print('Training process has finished.')

writer.close()

"""# Tensorboard
Sweet spot of 6 epochs
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=/content/runs

"""This shows overfitting after 6 epochs we can highlight the fact that model overfits if trained for higher epochs"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=/content/runs



